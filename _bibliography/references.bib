
@article{jin_sympnets_2020,
	title = {{SympNets}: {Intrinsic} structure-preserving symplectic networks for identifying {Hamiltonian} systems},
	volume = {132},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608020303063},
	doi = {https://doi.org/10.1016/j.neunet.2020.08.017},
	abstract = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
	journal = {Neural Networks},
	author = {Jin, Pengzhan and Zhang, Zhen and Zhu, Aiqing and Tang, Yifa and Karniadakis, George Em},
	year = {2020},
	keywords = {Deep learning, Dynamical systems, Hamiltonian systems, Physics-informed, Symplectic integrators, Symplectic maps},
	pages = {166--179},
}

@inproceedings{romero_attentive_2020,
	title = {Attentive {Group} {Equivariant} {Convolutional} {Networks}},
	url = {https://proceedings.mlr.press/v119/romero20a.html},
	abstract = {Although group convolutional networks are able to learn powerful representations based on symmetry patterns, they lack explicit means to learn meaningful relationships among them (e.g., relative positions and poses). In this paper, we present attentive group equivariant convolutions, a generalization of the group convolution, in which attention is applied during the course of convolution to accentuate meaningful symmetry combinations and suppress non-plausible, misleading ones. We indicate that prior work on visual attention can be described as special cases of our proposed framework and show empirically that our attentive group equivariant convolutional networks consistently outperform conventional group convolutional networks on benchmark image datasets. Simultaneously, we provide interpretability to the learned concepts through the visualization of equivariant attention maps.},
	language = {en},
	urldate = {2024-03-19},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Romero, David and Bekkers, Erik and Tomczak, Jakub and Hoogendoorn, Mark},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8188--8199}
	}

@misc{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1602.07576},
	doi = {10.48550/arXiv.1602.07576},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Cohen, Taco S. and Welling, Max},
	month = jun,
	year = {2016},
	note = {arXiv:1602.07576 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
	}

@inproceedings{villar_scalars_2021,
	title = {Scalars are universal: {Equivariant} machine learning, structured like classical physics},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/f1b0775946bc0329b35b823b86eeb5f5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Villar, Soledad and Hogg, David W and Storey-Fisher, Kate and Yao, Weichi and Blum-Smith, Ben},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {28848--28863},
}

@misc{wang_incorporating_2021,
	title = {Incorporating {Symmetry} into {Deep} {Dynamics} {Models} for {Improved} {Generalization}},
	url = {http://arxiv.org/abs/2002.03061},
	doi = {10.48550/arXiv.2002.03061},
	abstract = {Recent work has shown deep learning can accelerate the prediction of physical dynamics relative to numerical solvers. However, limited physical accuracy and an inability to generalize under distributional shift limit its applicability to the real world. We propose to improve accuracy and generalization by incorporating symmetries into convolutional neural networks. Specifically, we employ a variety of methods each tailored to enforce a different symmetry. Our models are both theoretically and experimentally robust to distributional shift by symmetry group transformations and enjoy favorable sample complexity. We demonstrate the advantage of our approach on a variety of physical dynamics including Rayleigh B{\textbackslash}'enard convection and real-world ocean currents and temperatures. Compared with image or text applications, our work is a significant step towards applying equivariant neural networks to high-dimensional systems with complex dynamics. We open-source our simulation, data, and code at {\textbackslash}url\{https://github.com/Rose-STL-Lab/Equivariant-Net\}.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Wang, Rui and Walters, Robin and Yu, Rose},
	month = mar,
	year = {2021},
	note = {arXiv:2002.03061 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Representation Theory}
	}
